\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

  \title{Summary of Meshtalk WIP}

  \author{Jay \\
    CAU EAI Lab \\
    Lab address Here\\
    {https://yukinyaa.github.io}
  }
  \maketitle
  \thispagestyle{empty}

  \begin{abstract}
    The article summarizes Meshtalk\cite{richard2021meshtalk}.
    The goal of this practice is to get familiar with \LaTeX{} and technical writing(hopefully).
  \end{abstract}


  \section{Introduction to Meshtalk}
    Meshtalk is a generic method for generating full facial mesh animation from speech. It can generate lip-sync animation from a single frame of generic human facial mesh, and also can mix in emotional information from mesh animation.

      
    \subsection{Dataset}
      \begin{table}
        \begin{center}
          \begin{tabular}{|c||c c|}
              \hline
                Data Class & Resolution & Frame rate \\
              \hline\hline
                Face Video & 80 cameras & 30 FPS \\
                Face Mesh & 1,672 vertices & 30 FPS \\
              \hline
                Audio & 16kHz & - \\
                Mel Spectrogram & 80 Dimension & 10ms(100 FPS) \\
              \hline
          \end{tabular}
        \end{center}
        \caption{Captured, then processed Datasets}
        \label{table:dataset}
      \end{table}
      In total, there are 1.4 million frames of  13 hours equivalent audio-visual dataset from 250 subjects, reading 50 phonetically balanced sentences.

      For training dataset, first 40 sentences out of 50 \textit{about} 200 out of 250 subjects, total $40\times 200$ dataset is used for training. For evaluation, remaining 10 sentences of 50 subjects are used.
      \subsubsection*{Mesh Dataset}
      Face motion is captured with synchronized cameras, and processed into high-detail mesh, including eyelid, hairstyle, etc.
      \subsubsection*{Audio Dataset}
      Audio data is recorded in 16kHz as shown on Table \ref{table:dataset}.
      For every mesh, 600ms of audio snippet is processed info Mel spectrogram, every 10ms, in 80 dimension. Hence, \(\mathsf{a}_t \in \mathbb{R}^{60 \times 80}\)      
 
    \subsection{Network Design}


    \begin{figure}[t]
      \begin{center}
      \includegraphics[width=0.8\linewidth]{meshtalk_overview.png}
      \end{center}
      \caption{The network diagram\cite{richard2021meshtalk}.}
      \label{fig:long}
      \label{fig:networkdiag}
    \end{figure}

    The network resembles Variational Autoencoder with multiple latent space as shown in Figure \ref{fig:networkdiag}.
    Target mesh \(\hat{h}\) is estimated from template mesh \(h\) and latent space \(\mathsf{c}\) by computing function \(\mathcal{D}\).
    \begin{equation}
      \hat{h}_{1:T} = \mathcal{D}(h, \mathsf{c}_{1:T, 1:H})
      \label{eq:1}
    \end{equation}
    Sequence of latent space \(\mathsf{c}_{1:T}\) is derived from audio sequence \(\mathsf{a}_{1:T}\) and expression signal mesh sequence \(\mathsf{x}_{1:T}\).
    \(c\) and \(a\) are first mapped to \(T*H*C\) dimensional latent space, then passed through Gumbel-softmax\cite{jang2017categorical} over every classification head.
    \begin{equation}
      \mathsf{c}_{1:T, 1:H} = [\mathsf{Gumbel}(\mathsf{enc}_{t,h,1:C})]_{1:T, 1:H}
    \end{equation}
    \begin{equation}
      \mathsf{enc}_{1:T,1:H,1:C} = \tilde{\xi}(x_{1:T}, a_{1:T}) 
    \end{equation}
  
    \pagebreak  
    
    \subsection{Training}
      The solution uses a novel cross-modality loss for calculating loss function for the network.
      \begin{equation}
        \begin{split}
          \mathcal{L}_{\mathsf{x}Mod} &= 
            \sum_{t=1}^{T}
            \sum_{v=1}^{V}
            \mathcal{M}_v^{\mathsf{(upper)}}
            (||\hat{h}_{t,v}^{\mathsf{(expr)}} - x_{t,v}||)\\
            &+
            \sum_{t=1}^{T}
            \sum_{v=1}^{V}
            \mathcal{M}_v^{\mathsf{(mouth)}}
            (||\hat{h}_{t,v}^{\mathsf{(audio)}} - x_{t,v}||)  
        \end{split}
      \end{equation}
      Where \(\hat{h}_{t,v}^{\mathsf{(expr)}}\) is estimated with correct expression signal and random speech signal, and
      \(\mathcal{M}^{(upper)}\) is a mask that assigned a higher weight to vertices around the mouth, and low weight to others.
      Correspondingly, \(\hat{h}_{t,v}^{\mathsf{(audio)}}\) is estimated with random expression signal and random speech signal.
      \(\mathcal{M}^{(mouth)}\) is a mask that assigns a lower weight around the mouth, and vice versa.
      Loss for eye is defined as following.
      \begin{equation}
          \mathcal{L}_{\mathsf{eyelid}} = 
            \sum_{t=1}^{T}
            \sum_{v=1}^{V}
            \mathcal{M}_v^{\mathsf{(eyelid)}}
            (||\hat{h}_{t,v}^{\mathsf{(expr)}} - x_{t,v}||)
      \end{equation}
      
      \(\mathcal{M}^{(eyelid)}\) is a specific eye loss, which was crucial.
      
      Final Loss term is defined as \(\mathcal{L} =\mathcal{L}_{\mathsf{x}Mod}+\mathcal{L}_{\mathsf{eyelid}} \), which gives both term equal weight.
  \section{Evaluation}
    To evaluate the network, the network is compared with  networks that lacks speech signal and uses \(\ell_2\) loss.
     \cite{Cudeiro_2019_CVPR}
      
    
%%%%%%%%%%%%%%%%%%%%%%% References %%%%%%%%%%%%%%%%%%%%%%%%%

{
  \small
  \bibliographystyle{ieee}
  \bibliography{ref}
}

\end{document}


